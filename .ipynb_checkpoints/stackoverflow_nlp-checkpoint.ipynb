{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation du notebook\n",
    "\n",
    "L'objectif du module est de développer un système de suggestion de tag pour le site Stack Overflow. Celui-ci prendra la forme d’un algorithme de machine learning qui assigne automatiquement plusieurs tags pertinents à une question.\n",
    "\n",
    "Ce notebook se consacre au processing des données textuelles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.auto_scroll_threshold=10\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob, os, pickle, warnings\n",
    "from sklearn.utils import resample\n",
    "import re, spacy, nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "# no recognition of dependency labels and named entities \n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])   \n",
    "#print(_nlp.pipe_names)\n",
    "#doc = nlp(\"successfully\")\n",
    "#for token in doc:\n",
    "#    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)\n",
    "\n",
    "# Create a Tokenizer with the default settings for English\n",
    "# including punctuation rules and exceptions\n",
    "spacy_tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
    "# Spacy va permettre de gérer ce genre de cas : i'm having issues. \n",
    "# Plutôt de renvoyer \"i\", \"m\", Spacy va renvoyer \"i\", \"'m\"\n",
    "# tokens = tokenizer(\"i'm having issues\")\n",
    "\n",
    "# Déclaration de variables \n",
    "data_path = r\"C:\\OCR\\06 _ Catégorisez automatiquement des questions\"\n",
    "   \n",
    "pd.options.mode.chained_assignment = None # default='warn'\n",
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHARGEMENT\n",
    "with open('alltags_counter.pkl', 'rb') as f:\n",
    "    alltags_counter = pickle.load(f) \n",
    "\n",
    "topntags = [tag[0] for tag in alltags_counter.most_common(100)]\n",
    "\n",
    "posts_body = pd.read_pickle('./posts_body.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing des données textuelles\n",
    "\n",
    "- Mise en minuscules du texte et suppression des caractères « whitespace » et du code\n",
    "- Suppression du format HTML avec le package Beautiful Soup \n",
    "- Recodage des top tags possédant des caractères spéciaux dans les documents\n",
    "- Suppression de la ponctuation\n",
    "- Suppression des stop words proposés par les modules NLP\n",
    "- Lemmatisation\n",
    "- Suppression des verbes et adjectifs qui n'apportent pas de valeur ajoutée pour ce que je souhaite faire (POS tagging)\n",
    "- Suppression de stop words manuels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_posts = resample(posts_body, n_samples=10, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avant nettoyage\n",
    "posts_body.loc[few_posts.index].sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_whitespace_and_code(text):\n",
    "    ''' Lowering text, removing whitespace and code \n",
    "    Parameter:\n",
    "    text: corpus to clean\n",
    "    '''\n",
    "\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub('\\s+', ' ', text) # matches all whitespace characters : \\t\\n\\r\\f\\v\n",
    "    text = re.sub('<code>[^<]*</code>', '', text)\n",
    "    text = text.strip(' ') # removes leading and trailing blanks\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_body_clean1 = posts_body.apply(lambda s: clean_whitespace_and_code(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# après nettoyage\n",
    "posts_body_clean1.loc[few_posts.index].sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_body_clean2 = posts_body_clean1.apply(lambda s: BeautifulSoup(s).get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# après nettoyage\n",
    "posts_body_clean2.loc[few_posts.index].sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des caractères distincts de la liste top500tags\n",
    "''.join(set().union(*topntags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'c++'\n",
    "reg = '(?u)\\S+'\n",
    "re.match(reg, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# certains mots des questions peuvent être des tags qui contiennent des signes de ponctuation, ils ne doivent pas être\n",
    "# affectés par le nettoyage des signes de ponctuation. Pour cela, j'établis une liste de correspondances dans laquelle \n",
    "# je fais correspondre à tous les tags de la liste des top tags contenant un signe de ponctuation, un mot clé généré. \n",
    "# Je remplace ensuite dans le texte tous les tags concernés par le mot clé généré correspondant. \n",
    "\n",
    "def create_specialtags_transco(toptags):\n",
    "    ''' create transcode list for tags with punctuation \n",
    "    Parameters:\n",
    "    toptags: whole list of top tags that contains tags with punctuation\n",
    "    '''    \n",
    "    \n",
    "    \n",
    "    regex = re.compile('[%s0-9]' % re.escape(punctuation))\n",
    "    # tags contenant des caractères spéciaux\n",
    "    specialtags = [tag for tag in toptags if regex.search(tag)]\n",
    "    specialtags_transco = []\n",
    "    for i in range(len(specialtags)):\n",
    "        specialtags_transco.append('xyzspecialtags'+str(i)+'zyx')\n",
    "    \n",
    "    regex = re.compile('^[a-z]+$')\n",
    "    ignore_words = [tag for tag in toptags if regex.search(tag)]\n",
    "    ignore_words.extend(specialtags_transco)\n",
    "    \n",
    "    return list(zip(specialtags, specialtags_transco)), ignore_words\n",
    "\n",
    "def apply_specialtags_transco(text, specialtags):\n",
    "    ''' Transcode tags with punctuation \n",
    "    Parameters:\n",
    "    text: text to transcode\n",
    "    '''    \n",
    "    \n",
    "    \n",
    "    for r in specialtags:\n",
    "        text = text.replace(*r)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specialtags, ignore_words = create_specialtags_transco(topntags)\n",
    "posts_body_clean3 = posts_body_clean2.apply(lambda s: apply_specialtags_transco(s, specialtags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specialtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# après nettoyage\n",
    "posts_body_clean3.loc[few_posts.index].sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(posts_body_clean3.loc[posts_body_clean3.str.contains('xyzspecialtags0zyx')].head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_punctuation(text): \n",
    "    ''' Remove punctuation\n",
    "    Parameters:\n",
    "    text: corpus to remove punctuation from it\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    regex = re.compile('[%s]' % re.escape(punctuation))\n",
    "    result = re.sub(regex, ' ', text)    \n",
    "    result = re.sub(' +', ' ', result) # remove duplicates whitespaces\n",
    "    \n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_body_clean4 = posts_body_clean3.apply(lambda s: clean_punctuation(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# après nettoyage\n",
    "posts_body_clean4.loc[few_posts.index].sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopWordsRemove(text, stop_words):\n",
    "    ''' Removing all the english stop words from a corpus\n",
    "    Parameter:\n",
    "    text: corpus to remove stop words from it\n",
    "    stop_words: list of stop words to exclude\n",
    "    '''\n",
    "\n",
    "    \n",
    "    words = spacy_tokenizer(text)\n",
    "    filtered = [str(w) for w in words if not str(w) in stop_words]\n",
    "    text = ' '.join(map(str, filtered))\n",
    "    result = re.sub(' +', ' ', text) # remove duplicates whitespaces\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(text_in, allowed_postags, ignore_words):\n",
    "    ''' It keeps the lemma of the words (lemma is the uninflected form of a word),\n",
    "    and deletes the undesired POS tags\n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "    text_in (list): text to lemmatize\n",
    "    allowed_postags (list): list of allowed postags, like NOUN, ADJ, VERB, ADV\n",
    "    ignore_words: list of words to include without processing them\n",
    "    '''\n",
    "\n",
    "    \n",
    "    doc = nlp(text_in) \n",
    "    text_out = []\n",
    "    \n",
    "    for token in doc:\n",
    "        \n",
    "        if str(token) in ignore_words:\n",
    "            text_out.append(str(token))\n",
    "            \n",
    "        elif token.pos_ in allowed_postags:            \n",
    "            text_out.append(token.lemma_)\n",
    "                \n",
    "    text_out = ' '.join(text_out)\n",
    "    result = re.sub(' +', ' ', text_out) # remove duplicates whitespaces\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il y a pas mal de mots (ex: like, use, get, new) qui n'apportent aucune information pertinente sur le contenu de la question. \n",
    "\n",
    "Je me focalise donc sur les noms essentiellement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_stopwords = set(set(nlp.Defaults.stop_words) | set(stopwords.words(\"english\")))\n",
    "posts_body_clean5 = posts_body_clean4.apply(lambda s: stopWordsRemove(s, auto_stopwords)) \n",
    "posts_body_clean6 = posts_body_clean5.apply(lambda s: lemmatization(s, ['NOUN'], ignore_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# après nettoyage\n",
    "posts_body_clean6.loc[few_posts.index].sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "# Graph showing the words most used in the post on Stackoverflow\n",
    "\n",
    "word_count = Counter()\n",
    "posts_body_clean6.apply(lambda x : word_count.update([s for s in x.split(' ') if s != ''])) ;\n",
    "\n",
    "inv_specialtags = {v: k for k, v in dict(specialtags).items()}\n",
    "for word in inv_specialtags.keys():\n",
    "    if word in word_count.keys():\n",
    "        word_count[inv_specialtags[word]] = word_count.pop(word)\n",
    "        \n",
    "result = pd.DataFrame(word_count.most_common(25),columns=['Word','Frequency'])\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.xlabel('Number of tags', fontsize=18)\n",
    "plt.ylabel('Number of posts', fontsize=18)\n",
    "plt.xticks(rotation=90, fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.bar(result.Word, result.Frequency)\n",
    "plt.title(\"Words most used in the posts on Stackoverflow\",fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(word_count.most_common(200),columns=['Word','Frequency'])\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le lot, il y a des mots \"fourre-tout\" que j'intègre aux stopwords et je réapplique la fonction de suppression des stopwords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_stopwords = pd.read_excel(\"./common_words.xls\").dropna(axis=0)['Word'].to_list()\n",
    "print(manual_stopwords)\n",
    "manual_stopwords = set(manual_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_body_clean7 = posts_body_clean6.apply(lambda s: stopWordsRemove(s, manual_stopwords)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "# Graph showing the words most used in the post on Stackoverflow\n",
    "\n",
    "word_count = Counter()\n",
    "posts_body_clean7.apply(lambda x : word_count.update([s for s in x.split(' ') if s != ''])) ;\n",
    "\n",
    "inv_specialtags = {v: k for k, v in dict(specialtags).items()}\n",
    "for word in inv_specialtags.keys():\n",
    "    if word in word_count.keys():\n",
    "        word_count[inv_specialtags[word]] = word_count.pop(word)\n",
    "        \n",
    "result = pd.DataFrame(word_count.most_common(25),columns=['Word','Frequency'])\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.xlabel('Number of tags', fontsize=18)\n",
    "plt.ylabel('Number of posts', fontsize=18)\n",
    "plt.xticks(rotation=90, fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.bar(result.Word, result.Frequency)\n",
    "plt.title(\"Words most used in the posts on Stackoverflow\",fontsize=20)\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# je compte les occurrences de chaque tag dans le dataframe\n",
    "from collections import Counter\n",
    "allwords_count = Counter()\n",
    "posts_body_clean7.apply(lambda s : allwords_count.update(s.split(' '))) ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_body_clean7 = posts_body_clean7.dropna(axis=0)\n",
    "print('Taille finale du dataframe : {}'.format(posts_body_clean7.shape[0])) # NB: pas de documents vides après NLP\n",
    "print('Nombre de mots distincts : {}'.format(len(allwords_count)))\n",
    "print('Nombre total de mots : {}'.format(sum(allwords_count.values())))\n",
    "print('Nombre moyen de mots par document : {:0.2f}'.format(sum(allwords_count.values())/posts_body_clean7.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAUVEGARDE\n",
    "with open('ignore_words.pkl', 'wb') as f:    \n",
    "    pickle.dump(ignore_words, f)\n",
    "with open('specialtags.pkl', 'wb') as f:    \n",
    "    pickle.dump(specialtags, f)\n",
    "with open('manual_stopwords.pkl', 'wb') as f:    \n",
    "    pickle.dump(set(manual_stopwords), f)    \n",
    "    \n",
    "posts_body_clean7.to_pickle(\"./posts_body_clean.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
